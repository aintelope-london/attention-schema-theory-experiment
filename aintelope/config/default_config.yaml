# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.
#
# Repository:
# https://github.com/aintelope-london/attention-schema-theory-experiment     

run:
  outputs_dir: outputs/${now:%Y%m%d%H%M%S}/
  seed: 0                          # @ui int 0 9999
  max_workers: 0                   # @ui int -1 100
  event_columns: [
    Run_id, Trial, Episode, Env layout seed, Step, IsTest,
    Agent_id, State, Action, Reward, Done, Next_state, Observation]
  save_logs: True                  # @ui bool
  trials: 1                        # @ui int 1 100
  episodes: 1                      # @ui int 1 100000
  steps: 10                        # @ui int 1 10000
  test_mode: False                 # @ui bool
agent_params:
  save_frequency: ${muldiv:${run.episodes},${run.steps},10}
  roi_mode: "cone"                 # @ui str cone  
  start_with_model: False          # @ui bool
  use_previous_model: True         # @ui bool
  batch_size: 20                   # @ui int 1 100
  gamma: 0.7                       # @ui float 0.0 1.0
  # Agents
  agent_0:
    agent_class: random_agent      # @ui str main_agent,rangom_agent,sb3_ppo_agent,sb3_a2c_agent
    architecture: 
      reward: [RewardInference]    # @ui str [RewardInference]
      dynamic: NextState-NN        # @ui str None,NextState-NN
      value: StateValue-NN         # @ui str None,StateValue-NN
      action: ModelBased           # @ui str None,ModelBased
  # SB3 model parameters
  num_conv_layers: 0               # @ui int 0 3
  learning_rate: 0.0003            # @ui float 0.0001 0.01
  ppo_n_steps: 64                  # @ui int 32 8192
env_params:
  env: savanna-safetygrid-v1  # @ui str savanna-safetygrid-v1
  env_layout_seed_repeat_sequence_length: -1  # @ui int -1 1000
  env_layout_seed_modulo: 0        # @ui int 0 1000
  mode: parallel         # @ui str serial,parallel
  combine_interoception_and_vision: True  # @ui bool
  interoception_transformation_mode: 0     # @ui int 0 4
  interoception_embedding_gaussian_centers: [-32, -16, -8, -4, -2, -1, 0, 1, 2, 4]
  interoception_embedding_gaussian_scales: [32, 16, 8, 4, 2, 1, 1, 1, 2, 4]
  num_iters: ${run.steps}
  env_experiment: ai_safety_gridworlds.aintelope_savanna
  level: 0                         # @ui int 0 10
  map_max: 5                       # @ui int 3 50
  map_width: ${env_params.map_max}
  map_height: ${env_params.map_max}
  render_agent_radius: ${minus_3:${env_params.map_max}}
  map_randomization_frequency: 3   # @ui int 0 3
  render_mode: null                # @ui str null,human,rgb_array
  amount_agents: 1
  amount_grass_patches: 1          # @ui int 0 20
  amount_water_holes: 0            # @ui int 0 20
  amount_danger_tiles: 0           # @ui int 0 20
  amount_predators: 0              # @ui int 0 10
  enable_homeostasis: False        # @ui bool
  sustainability_challenge: False  # @ui bool
  amount_gold_deposits: 0          # @ui int 0 20
  amount_silver_deposits: 0        # @ui int 0 20
  scores:
      DANGER_TILE_SCORE: '{"INJURY": 0}'             # @ui str
      PREDATOR_NPC_SCORE: '{"INJURY": 0}'            # @ui str
      MOVEMENT_SCORE: '{"MOVEMENT": 0}'              # @ui str
      COOPERATION_SCORE: '{"COOPERATION": 0}'        # @ui str
      GOLD_SCORE: '{"GOLD": 0}'                      # @ui str
      SILVER_SCORE: '{"SILVER": 0}'                  # @ui str
      FOOD_SCORE: '{"FOOD": 20}'                     # @ui str
      FOOD_DEFICIENCY_SCORE: '{"FOOD_DEFICIENCY": 0}'  # @ui str
      FOOD_OVERSATIATION_SCORE: '{"FOOD_OVERSATIATION": 0}'  # @ui str
      DRINK_SCORE: '{"DRINK": 0}'                    # @ui str
      DRINK_DEFICIENCY_SCORE: '{"DRINK_DEFICIENCY": 0}'  # @ui str
      DRINK_OVERSATIATION_SCORE: '{"DRINK_OVERSATIATION": 0}'  # @ui str
  FOOD_DEFICIENCY_INITIAL: 0       # @ui float -100.0 100.0
  FOOD_EXTRACTION_RATE: 0.25       # @ui float 0.0 10.0
  FOOD_DEFICIENCY_RATE: -0.1       # @ui float -10.0 0.0
  FOOD_OVERSATIATION_LIMIT: 4      # @ui float 0.0 100.0
  FOOD_OVERSATIATION_THRESHOLD: 2  # @ui float 0.0 100.0
  FOOD_DEFICIENCY_LIMIT: -20       # @ui float -100.0 0.0
  FOOD_DEFICIENCY_THRESHOLD: -3    # @ui float -100.0 0.0
  FOOD_GROWTH_LIMIT: 20            # @ui float 0.0 100.0
  FOOD_REGROWTH_EXPONENT: 1.1      # @ui float 0.1 10.0
  DRINK_DEFICIENCY_INITIAL: 0      # @ui float -100.0 100.0
  DRINK_EXTRACTION_RATE: 0.25      # @ui float 0.0 10.0
  DRINK_DEFICIENCY_RATE: -0.1      # @ui float -10.0 0.0
  DRINK_OVERSATIATION_LIMIT: 4     # @ui float 0.0 100.0
  DRINK_OVERSATIATION_THRESHOLD: 2 # @ui float 0.0 100.0
  DRINK_DEFICIENCY_LIMIT: -20      # @ui float -100.0 0.0
  DRINK_DEFICIENCY_THRESHOLD: -3   # @ui float -100.0 0.0
  DRINK_GROWTH_LIMIT: 20           # @ui float 0.0 100.0
  DRINK_REGROWTH_EXPONENT: 1.1     # @ui float 0.1 10.0
  GOLD_VISITS_LOG_BASE: 1.5        # @ui float 1.0 10.0
  SILVER_VISITS_LOG_BASE: 1.5      # @ui float 1.0 10.0
  PREDATOR_MOVEMENT_PROBABILITY: 0.25  # @ui float 0.0 1.0
  observation_direction_mode: 0    # @ui int 0 2
  action_direction_mode: 0         # @ui int 0 2
  test_death: False                # @ui bool
  seed: ${run.seed}
models:
    RewardInference:
      metadata:
        post_activate: True
    DQN:
      metadata:
        q_network: DQN-NN
        eps_start: 0.9
        eps_end: 0.05
        eps_decay_steps: 1000      
    ModelBased:
      metadata:
        input:
          dynamic: sae
          value: state_value
        num_simulations: 5
        max_depth: 2
        c_puct: 1.0
    DQN-NN:
      metadata:
        optimizer: AdamW
        optimizer_params:
          lr: 0.001
          amsgrad: True
          weight_decay: 0.01
        loss_function: SmoothL1Loss
        target: [reward]
        use_target_net: True
        target_net_update_frequency: 100
        tau: 0.05
      architecture:
        vision_net:
          - {type: input, source: vision}
          - {type: conv, size: 8, kernel: 1}
          - {type: relu}
          - {type: conv, size: 8, kernel: 3}
          - {type: relu}
          - {type: linear, size: 16}
          - {type: relu}
        interoception_net:
          - {type: input, source: interoception}
          - {type: linear, size: 4}
          - {type: relu}
        audio_net:
          - {type: input, source: audio}
          - {type: linear, size: 4}
          - {type: relu}
        combined_net:
          - {type: input, source: vision_net+audio_net+interoception_net}
          - {type: linear, size: 8}
          - {type: relu}
          - {type: linear, source: action}
          - {type: output}
    StateValue-NN:
      metadata:
        optimizer: Adam
        optimizer_params:
          lr: 0.0005
        loss_function: MSELoss
        target: [reward]
        use_target_net: False
      architecture:
        vision_net:
          - {type: input, source: vision}
          - {type: conv, size: 8, kernel: 1}
          - {type: relu}
          - {type: conv, size: 8, kernel: 3}
          - {type: relu}
          - {type: linear, size: 16}
          - {type: relu}
        interoception_net:
          - {type: input, source: interoception}
          - {type: linear, size: 4}
          - {type: relu}
        audio_net:
          - {type: input, source: audio}
          - {type: linear, size: 4}
          - {type: relu}
        combined_net:
          - {type: input, source: vision_net+audio_net+interoception_net}
          - {type: linear, size: 8}
          - {type: relu}
          - {type: linear, size: 1}
          - {type: output}
    NextState-NN:
      metadata:
        optimizer: Adam
        optimizer_params:
          lr: 0.001
        loss_function: MSELoss
        target: [next_observation]
        use_target_net: False
        post_activate: True # cannot be processed without action during get_action! TEMP 
      architecture:
        vision_net:
          - {type: input, source: vision}
          - {type: conv, size: 8, kernel: 1}
          - {type: relu}
          - {type: conv, size: 8, kernel: 3}
          - {type: relu}
          - {type: linear, size: 16}
          - {type: relu}
        interoception_net:
          - {type: input, source: interoception}
          - {type: linear, size: 4}
          - {type: relu}
        audio_net:
          - {type: input, source: audio}
          - {type: linear, size: 4}
          - {type: relu}
        encoder:
          - {type: input, source: vision_net+audio_net+interoception_net+action}
          - {type: linear, size: 32}
          - {type: relu}
          - {type: linear, size: 16}
          - {type: relu}
          - {type: linear, size: 32}
        vision_denet:
          - {type: input, source: encoder}
          - {type: linear, source: vision_encoded}
          - {type: unflatten} #, size: [5, 5, 8]}  
          - {type: conv_transpose, size: 8, kernel: 3}  
          - {type: relu}
          - {type: conv_transpose, size: 3, kernel: 1}  
          - {type: output}
        interoception_denet:
          - {type: input, source: encoder}
          - {type: relu}
          - {type: linear, size: 4}
          - {type: linear, source: interoception}
          - {type: output}
        audio_denet:
          - {type: input, source: encoder}
          - {type: relu}         
          - {type: linear, size: 4}
          - {type: linear, source: audio}
          - {type: output}
